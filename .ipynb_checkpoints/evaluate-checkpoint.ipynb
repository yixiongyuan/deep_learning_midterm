{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57d9a13-a717-43ca-89d9-ccd52d5eaffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is for pruned model fine-tune training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf6162a-4582-4df1-b4fb-3100d2a2bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import useful package\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import models\n",
    "from torchinfo import summary\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f40f2fb-68b6-43a9-8da4-a7dc276ecb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set up device \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6871a1-26c5-4014-ab32-a178b28575d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239f3624-5e39-42a6-b6c6-096c7ab0449f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model after pruning\n",
    "path = './checkpoint/pruned.pth.tar'\n",
    "checkpoint = torch.load(path)\n",
    "model = models.resnet(cfg=checkpoint['cfg'])\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b50e32-20c0-45d6-afa6-73266d1ddd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4558938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "resnet                                   [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,728\n",
       "├─Sequential: 1-2                        [1, 64, 32, 32]           --\n",
       "│    └─FusionBlock: 2-1                  [1, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-1              [1, 64, 32, 32]           4,928\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 32, 32]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-4              [1, 64, 32, 32]           4,928\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 32, 32]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 32, 32]           --\n",
       "│    └─FusionBlock: 2-2                  [1, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-7              [1, 64, 32, 32]           4,928\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 32, 32]           128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-10             [1, 64, 32, 32]           4,928\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 32, 32]           128\n",
       "│    │    └─Sequential: 3-12             [1, 64, 32, 32]           102,528\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 32, 32]           --\n",
       "├─Sequential: 1-3                        [1, 128, 16, 16]          --\n",
       "│    └─FusionBlock: 2-3                  [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-14             [1, 128, 16, 16]          9,216\n",
       "│    │    └─BatchNorm2d: 3-15            [1, 128, 16, 16]          256\n",
       "│    │    └─ReLU: 3-16                   [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-17             [1, 128, 16, 16]          18,048\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-19             [1, 128, 16, 16]          8,192\n",
       "│    │    └─ReLU: 3-20                   [1, 128, 16, 16]          --\n",
       "│    └─FusionBlock: 2-4                  [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-21             [1, 128, 16, 16]          18,048\n",
       "│    │    └─BatchNorm2d: 3-22            [1, 128, 16, 16]          256\n",
       "│    │    └─ReLU: 3-23                   [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-24             [1, 128, 16, 16]          18,048\n",
       "│    │    └─BatchNorm2d: 3-25            [1, 128, 16, 16]          256\n",
       "│    │    └─Sequential: 3-26             [1, 128, 16, 16]          409,856\n",
       "│    │    └─ReLU: 3-27                   [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-4                        [1, 256, 8, 8]            --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 8, 8]            --\n",
       "│    │    └─BatchNorm2d: 3-28            [1, 128, 16, 16]          256\n",
       "│    │    └─channel_selection: 3-29      [1, 123, 16, 16]          128\n",
       "│    │    └─ReLU: 3-30                   [1, 123, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-31                 [1, 255, 8, 8]            282,285\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 255, 8, 8]            510\n",
       "│    │    └─ReLU: 3-33                   [1, 255, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-34                 [1, 256, 8, 8]            587,520\n",
       "│    │    └─Sequential: 3-35             [1, 256, 8, 8]            32,768\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 8, 8]            --\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 256, 8, 8]            512\n",
       "│    │    └─channel_selection: 3-37      [1, 220, 8, 8]            256\n",
       "│    │    └─ReLU: 3-38                   [1, 220, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-39                 [1, 254, 8, 8]            502,920\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 254, 8, 8]            508\n",
       "│    │    └─ReLU: 3-41                   [1, 254, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-42                 [1, 256, 8, 8]            585,216\n",
       "├─Sequential: 1-5                        [1, 512, 4, 4]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 4, 4]            --\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 256, 8, 8]            512\n",
       "│    │    └─channel_selection: 3-44      [1, 199, 8, 8]            256\n",
       "│    │    └─ReLU: 3-45                   [1, 199, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 167, 4, 4]            299,097\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 167, 4, 4]            334\n",
       "│    │    └─ReLU: 3-48                   [1, 167, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 4, 4]            769,536\n",
       "│    │    └─Sequential: 3-50             [1, 512, 4, 4]            131,072\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 4, 4]            --\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 512, 4, 4]            1,024\n",
       "│    │    └─channel_selection: 3-52      [1, 68, 4, 4]             512\n",
       "│    │    └─ReLU: 3-53                   [1, 68, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-54                 [1, 144, 4, 4]            88,128\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 144, 4, 4]            288\n",
       "│    │    └─ReLU: 3-56                   [1, 144, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-57                 [1, 512, 4, 4]            663,552\n",
       "├─BatchNorm2d: 1-6                       [1, 512, 4, 4]            1,024\n",
       "├─channel_selection: 1-7                 [1, 329, 4, 4]            512\n",
       "├─ReLU: 1-8                              [1, 329, 4, 4]            --\n",
       "├─AvgPool2d: 1-9                         [1, 329, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 10]                   3,300\n",
       "==========================================================================================\n",
       "Total params: 4,558,938\n",
       "Trainable params: 4,558,938\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 407.84\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 17.17\n",
       "Params size (MB): 18.24\n",
       "Estimated Total Size (MB): 35.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model paramter display\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(params)\n",
    "summary(model, input_size=(1,3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee0d45a-219e-4e68-8256-1df8d78dfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20669bc5-a259-406c-9423-ac4552e8d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function\n",
    "def train(epoch,train_loss_list,train_acc_list):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss +=loss.item()\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        loss.backward()\n",
    "          \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()/len(data)))\n",
    "                  \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    print('\\nTraining Set: Average loss: {:.4f},Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "            train_loss, correct, len(train_loader.dataset), 100. * train_acc))\n",
    "\n",
    "# test function\n",
    "def test(test_loss_list,test_acc_list):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_acc = correct / float(len(test_loader.dataset))\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),100. *test_acc))\n",
    "        \n",
    "        return correct / float(len(test_loader.dataset))\n",
    "\n",
    "def save_checkpoint(state, is_best, filepath):\n",
    "    torch.save(state, os.path.join(filepath, 'finetune.pth.tar'))\n",
    "    if is_best:\n",
    "        print('\\n!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \\n')\n",
    "        shutil.copyfile(os.path.join(filepath, 'finetune.pth.tar'), os.path.join(filepath, 'finetune_model_best.pth.tar'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6606d8-cc89-4dad-9312-d3c276fbefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax=plt.axes()\n",
    "plt.xlabel('Round')  \n",
    "plt.ylabel('Loss')   \n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
    "plt.plot(range(len(test_loss)), test_loss, label=\"test loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"loss_curve\")\n",
    "plt.savefig(\"./checkpoint/finetune/finetune_loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cda568-b671-4be1-93cf-c79dbf1bd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax=plt.axes()\n",
    "plt.xlabel('Round')    \n",
    "plt.ylabel('acc')     \n",
    "\n",
    "plt.plot(range(len(train_acc)), train_acc, label=\"train acc\")\n",
    "plt.plot(range(len(test_acc)), test_acc, label=\"test acc\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"acc_curve\")\n",
    "plt.savefig(\"./checkpoint/finetune/finetune_loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282b02d-038a-4ab3-80f6-031385b17ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 0.026661\n",
      "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.014692\n",
      "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.014431\n",
      "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.012937\n",
      "\n",
      "Training Set: Average loss: 0.0138,Accuracy: 16373/50000 (32.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0182, Accuracy: 3648/10000 (36.5%)\n",
      "\n",
      "One epoch time cost: 43.732 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.011058\n",
      "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.010722\n",
      "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.009017\n",
      "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.009092\n",
      "\n",
      "Training Set: Average loss: 0.0101,Accuracy: 26251/50000 (52.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0140, Accuracy: 4925/10000 (49.2%)\n",
      "\n",
      "One epoch time cost: 44.267 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.009998\n",
      "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.008014\n",
      "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.008481\n",
      "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.007291\n",
      "\n",
      "Training Set: Average loss: 0.0082,Accuracy: 30954/50000 (61.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0111, Accuracy: 6108/10000 (61.1%)\n",
      "\n",
      "One epoch time cost: 44.780 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.007665\n",
      "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.007017\n",
      "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.006263\n",
      "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.006159\n",
      "\n",
      "Training Set: Average loss: 0.0070,Accuracy: 34107/50000 (68.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0113, Accuracy: 6358/10000 (63.6%)\n",
      "\n",
      "One epoch time cost: 45.376 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.007005\n",
      "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.006027\n",
      "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.005638\n",
      "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.006045\n",
      "\n",
      "Training Set: Average loss: 0.0061,Accuracy: 36501/50000 (73.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0096, Accuracy: 6819/10000 (68.2%)\n",
      "\n",
      "One epoch time cost: 46.090 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.004396\n",
      "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.006088\n",
      "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.005552\n",
      "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.005937\n",
      "\n",
      "Training Set: Average loss: 0.0055,Accuracy: 37744/50000 (75.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0082, Accuracy: 7255/10000 (72.5%)\n",
      "\n",
      "One epoch time cost: 46.142 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.003906\n",
      "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.006187\n",
      "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.006099\n",
      "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.005436\n",
      "\n",
      "Training Set: Average loss: 0.0050,Accuracy: 38943/50000 (77.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0072, Accuracy: 7605/10000 (76.1%)\n",
      "\n",
      "One epoch time cost: 46.208 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.004749\n",
      "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.004666\n",
      "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.007292\n",
      "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.003899\n",
      "\n",
      "Training Set: Average loss: 0.0048,Accuracy: 39612/50000 (79.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 7154/10000 (71.5%)\n",
      "\n",
      "One epoch time cost: 46.375 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.004018\n",
      "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.005347\n",
      "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 0.004089\n",
      "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.005567\n",
      "\n",
      "Training Set: Average loss: 0.0045,Accuracy: 40313/50000 (80.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0065, Accuracy: 7837/10000 (78.4%)\n",
      "\n",
      "One epoch time cost: 45.829 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.004748\n",
      "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 0.004899\n",
      "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 0.005080\n",
      "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 0.005103\n",
      "\n",
      "Training Set: Average loss: 0.0043,Accuracy: 40448/50000 (80.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0055, Accuracy: 8115/10000 (81.2%)\n",
      "\n",
      "One epoch time cost: 45.993 s \n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!! NEW BEST ACC !!!!!!!!!!!!! \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0.0%)]\tLoss: 0.003708\n",
      "Train Epoch: 10 [12800/50000 (25.6%)]\tLoss: 0.005747\n",
      "Train Epoch: 10 [25600/50000 (51.2%)]\tLoss: 0.003485\n",
      "Train Epoch: 10 [38400/50000 (76.7%)]\tLoss: 0.003782\n",
      "\n",
      "Training Set: Average loss: 0.0041,Accuracy: 41047/50000 (82.1%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0068, Accuracy: 7719/10000 (77.2%)\n",
      "\n",
      "One epoch time cost: 46.239 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 11 [0/50000 (0.0%)]\tLoss: 0.004177\n",
      "Train Epoch: 11 [12800/50000 (25.6%)]\tLoss: 0.003885\n",
      "Train Epoch: 11 [25600/50000 (51.2%)]\tLoss: 0.002622\n",
      "Train Epoch: 11 [38400/50000 (76.7%)]\tLoss: 0.003737\n",
      "\n",
      "Training Set: Average loss: 0.0040,Accuracy: 41211/50000 (82.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0058, Accuracy: 8065/10000 (80.7%)\n",
      "\n",
      "One epoch time cost: 46.022 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 12 [0/50000 (0.0%)]\tLoss: 0.004442\n",
      "Train Epoch: 12 [12800/50000 (25.6%)]\tLoss: 0.003035\n",
      "Train Epoch: 12 [25600/50000 (51.2%)]\tLoss: 0.003454\n",
      "Train Epoch: 12 [38400/50000 (76.7%)]\tLoss: 0.003876\n",
      "\n",
      "Training Set: Average loss: 0.0039,Accuracy: 41393/50000 (82.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0085, Accuracy: 7391/10000 (73.9%)\n",
      "\n",
      "One epoch time cost: 45.852 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 13 [0/50000 (0.0%)]\tLoss: 0.003244\n",
      "Train Epoch: 13 [12800/50000 (25.6%)]\tLoss: 0.002938\n",
      "Train Epoch: 13 [25600/50000 (51.2%)]\tLoss: 0.002904\n",
      "Train Epoch: 13 [38400/50000 (76.7%)]\tLoss: 0.003886\n",
      "\n",
      "Training Set: Average loss: 0.0038,Accuracy: 41628/50000 (83.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0073, Accuracy: 7604/10000 (76.0%)\n",
      "\n",
      "One epoch time cost: 45.933 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 14 [0/50000 (0.0%)]\tLoss: 0.003045\n",
      "Train Epoch: 14 [12800/50000 (25.6%)]\tLoss: 0.004147\n",
      "Train Epoch: 14 [25600/50000 (51.2%)]\tLoss: 0.003136\n",
      "Train Epoch: 14 [38400/50000 (76.7%)]\tLoss: 0.003901\n",
      "\n",
      "Training Set: Average loss: 0.0038,Accuracy: 41836/50000 (83.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0068, Accuracy: 7782/10000 (77.8%)\n",
      "\n",
      "One epoch time cost: 46.285 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 15 [0/50000 (0.0%)]\tLoss: 0.003858\n",
      "Train Epoch: 15 [12800/50000 (25.6%)]\tLoss: 0.004517\n",
      "Train Epoch: 15 [25600/50000 (51.2%)]\tLoss: 0.004186\n",
      "Train Epoch: 15 [38400/50000 (76.7%)]\tLoss: 0.003579\n",
      "\n",
      "Training Set: Average loss: 0.0037,Accuracy: 41906/50000 (83.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0067, Accuracy: 7822/10000 (78.2%)\n",
      "\n",
      "One epoch time cost: 46.004 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 16 [0/50000 (0.0%)]\tLoss: 0.003424\n",
      "Train Epoch: 16 [12800/50000 (25.6%)]\tLoss: 0.003463\n",
      "Train Epoch: 16 [25600/50000 (51.2%)]\tLoss: 0.003530\n",
      "Train Epoch: 16 [38400/50000 (76.7%)]\tLoss: 0.004383\n",
      "\n",
      "Training Set: Average loss: 0.0036,Accuracy: 42019/50000 (84.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 7881/10000 (78.8%)\n",
      "\n",
      "One epoch time cost: 45.770 s \n",
      "\n",
      "----------------------------------------------- \n",
      "\n",
      "Train Epoch: 17 [0/50000 (0.0%)]\tLoss: 0.002507\n",
      "Train Epoch: 17 [12800/50000 (25.6%)]\tLoss: 0.003824\n",
      "Train Epoch: 17 [25600/50000 (51.2%)]\tLoss: 0.002734\n"
     ]
    }
   ],
   "source": [
    "best_prec1 = 0.\n",
    "\n",
    "import time\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "# training \n",
    "for epoch in range(0, 100):\n",
    "    if epoch in [30, 50]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train(epoch,train_loss,train_acc)\n",
    "    prec1 = test(test_loss,test_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    interval = end_time - start_time\n",
    "    print('One epoch time cost: {:.3f} s \\n'.format(interval))\n",
    "   \n",
    "    \n",
    "    #scheduler.step()\n",
    "    \n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, filepath='./checkpoint')\n",
    "    \n",
    "    print('----------------------------------------------- \\n')\n",
    "\n",
    "    \n",
    "print(\"Best accuracy: \"+str(best_prec1))\n",
    "\n",
    "# write loss data to txt file\n",
    "savepath = os.path.join('./checkpoint/finetune', \"finetune_train_loss.txt\")\n",
    "with open(savepath, \"w\") as fp:\n",
    "    for data in train_loss:\n",
    "        fp.write(str(data)+\"\\n\")\n",
    "    \n",
    "savepath = os.path.join('./checkpoint/finetune', \"finetune_train_acc.txt\")\n",
    "with open(savepath, \"w\") as fp:\n",
    "       for data in train_acc:\n",
    "        fp.write(str(data)+\"\\n\")\n",
    "    \n",
    "savepath = os.path.join('./checkpoint/finetune', \"finetune_test_loss.txt\")\n",
    "with open(savepath, \"w\") as fp:\n",
    "    for data in test_loss:\n",
    "        fp.write(str(data)+\"\\n\")\n",
    "    \n",
    "savepath = os.path.join('./checkpoint/finetune', \"finetune_test_acc.txt\")\n",
    "with open(savepath, \"w\") as fp:\n",
    "    for data in test_acc:\n",
    "        fp.write(str(data)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a9e97-7fb0-4943-90b3-ad92c12b5da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
